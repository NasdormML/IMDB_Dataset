{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer, AdamW\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Настройки\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\n\n# 1. Подготовка данных\ndataset = load_dataset(\"imdb\")\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\nclass SentimentDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        label = self.data[idx][\"label\"]\n\n        encoding = self.tokenizer(\n            text,\n            max_length=MAX_LEN,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }\n\n# 2. Определение модели BERT для классификации\nclass BertSentiment(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        # Используем [CLS]-токен (первый токен) для классификации\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        return self.classifier(pooled_output)\n\n# 3. Функция обучения модели\ndef train_model(model, train_loader, val_loader):\n    model.to(DEVICE)\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = batch[\"label\"].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        # Валидация после каждой эпохи\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch[\"input_ids\"].to(DEVICE)\n                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n                labels = batch[\"label\"].to(DEVICE)\n                outputs = model(input_ids, attention_mask)\n                preds = torch.argmax(outputs, dim=1)\n                val_preds.extend(preds.cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n\n        acc = accuracy_score(val_labels, val_preds)\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Validation Accuracy: {acc:.4f}\")\n\n# 4. Функция оценки модели\ndef evaluate_model(model, data_loader):\n    model.eval()\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = batch[\"label\"].to(DEVICE)\n            outputs = model(input_ids, attention_mask)\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(\"\\n\" + \"=\"*50)\n    print(\"Final Evaluation Metrics\")\n    print(\"=\"*50)\n    print(f\"Accuracy: {accuracy:.4f}\\n\")\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_preds))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(all_labels, all_preds))\n    return accuracy\n\n# 5. Основной блок: подготовка данных, обучение и оценка модели\nif __name__ == \"__main__\":\n    # Создание датасетов и загрузчиков\n    train_dataset = SentimentDataset(dataset[\"train\"], tokenizer)\n    val_dataset = SentimentDataset(dataset[\"test\"], tokenizer)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n    # Инициализация и обучение модели\n    model = BertSentiment()\n    print(\"Training BERT model...\")\n    train_model(model, train_loader, val_loader)\n\n    # Оценка модели на тестовом наборе\n    print(\"\\nEvaluating model...\")\n    evaluate_model(model, val_loader)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:42:07.709860Z","iopub.execute_input":"2025-02-16T14:42:07.710099Z","iopub.status.idle":"2025-02-16T15:17:49.126910Z","shell.execute_reply.started":"2025-02-16T14:42:07.710058Z","shell.execute_reply":"2025-02-16T15:17:49.125814Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"994aa4a0bc5e4fb8b94ebaa4abc59ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a113e6229645f6bba2ea46084b9461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2507bdca1b041948a399dd91efca3a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7899caccbe554d349432a5deeef1ee86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b2f707aff8419ca134ea6f2f844600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1390fdc0b52d4dbfa499cd723b733c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75016914520b4c75af7edaa973a272cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3f1f10e16646e08b1df88264b997ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e10d7aa9384b5dbf1018fc9811b1de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"217531cb342c45de8f4a953a15e4a0f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"045e89fee62a43f19675d73714bfccec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845e0847d7424fe7bbdb6e5da8e4d2a9"}},"metadata":{}},{"name":"stdout","text":"Training BERT model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 | Validation Accuracy: 0.8805\nEpoch 2/3 | Validation Accuracy: 0.8864\nEpoch 3/3 | Validation Accuracy: 0.8849\n\nEvaluating model...\n\n==================================================\nFinal Evaluation Metrics\n==================================================\nAccuracy: 0.8849\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.90      0.89     12500\n           1       0.89      0.87      0.88     12500\n\n    accuracy                           0.88     25000\n   macro avg       0.89      0.88      0.88     25000\nweighted avg       0.89      0.88      0.88     25000\n\nConfusion Matrix:\n[[11190  1310]\n [ 1567 10933]]\n","output_type":"stream"}],"execution_count":1}]}